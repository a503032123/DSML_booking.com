{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“DSML_104403041_A8.ipynb”",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a503032123/DSML_booking.com/blob/master/%E2%80%9CDSML_104403041_A8_ipynb%E2%80%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-IIPt1LngyC",
        "colab_type": "text"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u1p2pzGnncS",
        "colab_type": "text"
      },
      "source": [
        "Try to imporvement the experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2CrdftZpAqZ",
        "colab_type": "text"
      },
      "source": [
        "**last time:** \n",
        "\n",
        "Model:Naive Bayes\n",
        "\n",
        "\n",
        "Feature engineering:tfidf, word2vec "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02hpdafUph2Y",
        "colab_type": "text"
      },
      "source": [
        "**Methods I try**\n",
        "\n",
        "\n",
        "change model about CNN\n",
        "\n",
        "Try POStag,PCA,NamedEntity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5jny96qqMRE",
        "colab_type": "text"
      },
      "source": [
        "===========================================================================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb0kgBzNnb0d",
        "colab_type": "code",
        "outputId": "5ca45062-8bb5-4a0e-b4a7-bdaf6793e6be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1128
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"popular\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing\n",
        "\n",
        "!pip3 install pydrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.7.9)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.4.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.0.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (3.1.1)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljrAeESEZou7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the training data.\n",
        "\n",
        "#Firstly, conducting the authentication on google drive. \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cC9U5E5auHG",
        "colab_type": "code",
        "outputId": "e7d1b4c5-887e-4ecc-90b2-8d72fa17211d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "drive = GoogleDrive(gauth) #Create GoogleDrive instance with authenticated GoogleAuth instance\n",
        "\n",
        "#See my file in A2 folder(1RHLLHMpY....)\n",
        "file_list = drive.ListFile({'q': \"'1snGpUVTrGOHMmaeOXdfZfej3p9tXdsSP' in parents and trashed=false\"}).GetList()\n",
        "for file in file_list:\n",
        "  print('title: %s, id: %s' % (file['title'], file['id']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: Final_Paper.pdf, id: 1IRdlDOoiceT38f6u7viWyxUcmWpYRqx0\n",
            "title: clean_dataset.csv, id: 1XycH_rePLI3VcUc74gHu1CKSQUBNajK6\n",
            "title: Final_Paper.doc, id: 136tivaDvC-Gu7mJigAZf9sjN54ePpUO-\n",
            "title: Experiment Results.xlsx, id: 1xi3lcZbsbF7gzmpp80YrvjhO6emaPl9t\n",
            "title: notebook, id: 1Dz8GOy7pV2roku2HBp3JdYpBrrVQ6a9W\n",
            "title: figure, id: 1mhU-7FPdx8glFhkjB0CCqnEvbAWl9gdR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTMtprXeZXFT",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qEEKPZfZXFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_from_gdrive = drive.CreateFile({'id': '1XycH_rePLI3VcUc74gHu1CKSQUBNajK6'})\n",
        "csv_from_gdrive.GetContentFile('data.csv')\n",
        "data = pd.read_csv('data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9bXkj5LZXFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Missing value: Drop the reviews with missing value directly.\n",
        "data.dropna(inplace = True)\n",
        "data.drop('Unnamed: 0', 1, inplace =True)\n",
        "data.reset_index(drop = True, inplace = True)\n",
        "#500717 records"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9m_WjvvZXFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#One-side Reviews: Remove the review with \"No Negative\"/\"No Positive\". \n",
        "data['NegativeReview'].replace('No Negative', \"+\", inplace = True)\n",
        "data['PositiveReview'].replace('No Positive', \"-\", inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va9iVdECZXFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Combine: Positive and negative reviews would be treated as only a review, and in addition : lower the case. \n",
        "corpus = data.NegativeReview + data.PositiveReview\n",
        "data.insert(0, \"Review\", corpus.str.lower())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTOVUoKrOg49",
        "colab_type": "code",
        "outputId": "164faf6e-423b-49b4-8792-2b9acfc9ee2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#delete not use coloum\n",
        "data.drop('NegativeReview', 1, inplace =True)\n",
        "data.drop('PositiveReview', 1, inplace =True)\n",
        "list(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Review', 'TripStyle']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU0LF7z83VJ_",
        "colab_type": "text"
      },
      "source": [
        "資料量過大 電腦無法處理因此隨機取出其中的四成做實驗"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqO74spJ2Xg8",
        "colab_type": "code",
        "outputId": "d40a9a7f-c6df-4e6c-fe49-e7006ff9c532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x=data\n",
        "x_train, x_test = train_test_split(x,test_size = 0.6, shuffle = True)\n",
        "data=x_train\n",
        "print(len(data.Review))\n",
        "#200286 records"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xL_R4wLZXFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization: Conduct the work_tokenize first. (sent_tokenizing is more complicated in this case.)\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "#data = data.join(data.TEXT.apply(sent_tokenize).rename('SENTENCES'))\n",
        "#sent = data.NegativeReview.apply(sent_tokenize)\n",
        "word = data.Review.apply(word_tokenize)\n",
        "data.insert(0, 'WordToken', word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_MoAoHa7zMj",
        "colab_type": "code",
        "outputId": "096e04cc-6bbb-4433-91ec-6bae241127ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#Anomaly, Weird records: Drop the  review   remove the review with less than 5 words\n",
        "word_count = data.WordToken.apply(lambda x: len(x))\n",
        "filter_count = (word_count >= 1)\n",
        "data = data[filter_count]\n",
        "data.reset_index(drop = True, inplace = True)\n",
        "#Now the total records are 500487\n",
        "print(len(data.Review))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJqwCUqeZXFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#StopWordRemoval: Remove the NLTK build-in stopwords in all the records.\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "wosw = data.WordToken.apply(lambda x: [ item for item in x if item not in stop_words ])\n",
        "#Casefold: In order to do the vectorization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CweL0LUbAAkl",
        "colab_type": "code",
        "outputId": "74b991b7-4337-49b5-df09-a6cb885e9f38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1045
        }
      },
      "source": [
        "#postag\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from nltk import ne_chunk ,pos_tag\n",
        "#pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "data2= data.WordToken.apply(pos_tag)\n",
        "#data2=nltk.pos_tag(word)\n",
        "#data2=ne_chunk(pos_tag(word_tokenize(data.Review)))\n",
        "\n",
        "data.insert(0, 'pos', data2)\n",
        "print(data.pos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0         [(the, DT), (equipment, NN), (in, IN), (the, D...\n",
            "1         [(the, DT), (breakfast, NN), (waitresses, VBZ)...\n",
            "2         [(the, DT), (staff, NN), (were, VBD), (friendl...\n",
            "3         [(would, MD), (have, VB), (been, VBN), (nice, ...\n",
            "4         [(some, DT), (staff, NN), (a, DT), (bit, NN), ...\n",
            "5         [(lots, NNS), (of, IN), (space, NN), (but, CC)...\n",
            "6         [(some, DT), (of, IN), (the, DT), (workers, NN...\n",
            "7         [(the, DT), (location, NN), (quite, RB), (a, D...\n",
            "8         [(+, IN), (the, DT), (staff, NN), (were, VBD),...\n",
            "9         [(awful, JJ), (bed, VBD), (very, RB), (poor, J...\n",
            "10                                  [(none, NN), (all, DT)]\n",
            "11        [(the, DT), (staff, NN), (were, VBD), (quite, ...\n",
            "12        [(wifi, JJ), (connection, NN), (was, VBD), (ve...\n",
            "13                                 [(price, NN), (all, DT)]\n",
            "14        [(+, IN), (the, DT), (staff, NN), (was, VBD), ...\n",
            "15        [(+, VB), (this, DT), (hotel, NN), (was, VBD),...\n",
            "16        [(+, JJ), (kattoterassi, NN), (uima, JJ), (alt...\n",
            "17        [(room, NN), (small, JJ), (bed, NN), (was, VBD...\n",
            "18        [(+, NN), (only, RB), (an, DT), (overnight, NN...\n",
            "19        [(little, JJ), (trouble, NN), (with, IN), (hou...\n",
            "20        [(+, IN), (all, PDT), (the, DT), (staff, NN), ...\n",
            "21        [(it, PRP), (can, MD), (be, VB), (crazy, JJ), ...\n",
            "22        [(+, IN), (the, DT), (room, NN), (was, VBD), (...\n",
            "23        [(the, DT), (worst, JJS), (bed, NN), (we, PRP)...\n",
            "24        [(room, NN), (on, IN), (the, DT), (street, NN)...\n",
            "25        [(nothing, NN), (everything, NN), (is, VBZ), (...\n",
            "26        [(no, DT), (cons, NNS), (come, VBP), (to, TO),...\n",
            "27        [(everything, NN), (was, VBD), (nice, JJ), (th...\n",
            "28        [(lifts, NNS), (were, VBD), (always, RB), (bus...\n",
            "29        [(some, DT), (staff, NN), (gave, VBD), (some, ...\n",
            "                                ...                        \n",
            "200231    [(nothing, NN), (the, DT), (best, JJS), (hotel...\n",
            "200232    [(parking, VBG), (not, RB), (near, IN), (the, ...\n",
            "200233    [(the, DT), (room, NN), (was, VBD), (in, IN), ...\n",
            "200234    [(nothing, NN), (service, NN), (modernness, NN...\n",
            "200235    [(disabled, JJ), (bathroom, NN), (facilities, ...\n",
            "200236    [(breakfast, NN), (was, VBD), (good, JJ), (but...\n",
            "200237    [(nothing, NN), (everything, NN), (was, VBD), ...\n",
            "200238    [(more, JJR), (facilities, NNS), (such, JJ), (...\n",
            "200239    [(room, NN), (rather, RB), (on, IN), (the, DT)...\n",
            "200240    [(internet, NN), (connection, NN), (could, MD)...\n",
            "200241    [(room, NN), (decor, NN), (slightly, RB), (out...\n",
            "200242    [(no, DT), (spinach, NN), (or, CC), (black, JJ...\n",
            "200243    [(+, IN), (the, DT), (concierge, NN), (head, N...\n",
            "200244    [(the, DT), (bed, NN), (the, DT), (bed, NN), (...\n",
            "200245    [(one, CD), (of, IN), (the, DT), (receptionist...\n",
            "200246    [(i, NN), (suspect, VBP), (that, IN), (the, DT...\n",
            "200247    [(+, JJ), (excellent, JJ), (hotel, NN), (with,...\n",
            "200248    [(+, JJ), (excellent, JJ), (service, NN), (fro...\n",
            "200249    [(reception, NN), (staff, NN), (could, MD), (b...\n",
            "200250    [(unfortunately, RB), (it, PRP), (was, VBD), (...\n",
            "200251          [(+, JJ), (location, NN), (restaurant, NN)]\n",
            "200252    [(+, JJ), (excellent, JJ), (bed, NN), (huge, J...\n",
            "200253    [(bad, JJ), (coffee, NN), (and, CC), (not, RB)...\n",
            "200254    [(wi, JJ), (fi, NN), (issues, NNS), (with, IN)...\n",
            "200255    [(none, NN), (regretted, VBD), (not, RB), (sta...\n",
            "200256    [(we, PRP), (were, VBD), (put, VBN), (in, IN),...\n",
            "200257    [(bed, NN), (was, VBD), (really, RB), (hard, J...\n",
            "200258    [(the, DT), (hotel, NN), (staff, NN), (were, V...\n",
            "200259    [(breakfast, NN), (cold, JJ), (very, RB), (qui...\n",
            "200260    [(nothing, NN), (staff, NN), (location, NN), (...\n",
            "Name: pos, Length: 200261, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1gPRyC1tSYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk to name entity but the tree is too big to convert\n",
        "\n",
        "#cp = nltk.RegexpParser(pattern)\n",
        "#cs = data.pos.apply(cp.parse)\n",
        "#prodata=nltk.pos_tag(data.WordToken)\n",
        "#cs = ne_chunk(data.pos)\n",
        "#iob_tagged = tree2conlltags(cs)\n",
        "#print(iob_tagged)\n",
        "#data.insert(0, 'iob_tagged',iob_tagged)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dUwTCiW1GnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#spacy for name entity\n",
        "#import spacy\n",
        "#from spacy import displacy\n",
        "#from collections import Counter\n",
        "#import en_core_web_sm\n",
        "#nlp = en_core_web_sm.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg7ij9Qg3GQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#doc = data.Review.apply(nlp)\n",
        "#data.insert(0, 'named_entity',doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdbleJxFBAX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(data.named_entity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxjGMdR2dz81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lemmatization: Convert the terms with different representations into the original.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "   return [wnl.lemmatize(w) for w in text]\n",
        "\n",
        "data.insert(0, 'Lemmatized',wosw.apply(lemmatize_text))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlmms8KHTcb3",
        "colab_type": "code",
        "outputId": "e5924ead-fb9a-4619-84b4-0e6594f7d463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#list(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pos', 'Lemmatized', 'WordToken', 'Review', 'TripStyle']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exj9EA89ZXFu",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgfleKjLJCo7",
        "colab_type": "text"
      },
      "source": [
        "use TFIDF and WordRmbedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acft9X30evlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vectorize I: Bow with TFIDF transformation.\n",
        "def dum(doc):\n",
        "    return doc\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(analyzer = 'word', \n",
        "                             tokenizer = dum,\n",
        "                             ngram_range = (1,1),\n",
        "                             min_df=5000,\n",
        "                             preprocessor = dum)\n",
        "x = vectorizer.fit_transform(data.WordToken)\n",
        "\n",
        "# 79567 dimensions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVukl8KhjuuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use name entity/postag fit\n",
        "x = vectorizer.fit_transform(data.pos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_QqnKdOey_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidfvec = x.toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiephUGhjSVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "le = LabelEncoder()\n",
        "x_ = tfidfvec\n",
        "lbl_ = le.fit_transform(data['TripStyle'])\n",
        "# 1 for Leisure trip, 0 for Business\n",
        "\n",
        "#lbl_ = to_categorical(lbl_)#使用CNN時lbl要再一次轉換Naive Bayes不用這行\n",
        "#Split them into train/test set, randomly with the test size 0.33 \n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, lbl_train, lbl_test = train_test_split(x_, lbl_ , test_size = 0.33, shuffle = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA5l-2xiDzEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vectorize II: WordEmbedding by Gensim.\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "model = Word2Vec(size = 300, window = 2, min_count = 1)\n",
        "#In order to count the single-word review, cfg with two word window and one count.\n",
        "\n",
        "#Building the dictionary.\n",
        "model.build_vocab(data.WordToken)\n",
        "\n",
        "#Train the w2v model with WordTokens\n",
        "model.train(data.WordToken, total_examples = len(data.WordToken), epochs = 1)\n",
        "model.init_sims(replace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1ZowIYyED6o",
        "colab_type": "code",
        "outputId": "c6d08bf2-5c07-423e-cb84-c6abcabac0af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#To lower the memory usage, save/load in KV model is necessary...\n",
        "fpath = get_tmpfile(\"w2v.kv\")\n",
        "model.wv.save(fpath)\n",
        "del model\n",
        "wv = KeyedVectors.load(fpath, mmap='r')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYlsYEI3Lmwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Document vector: By simply get the average of the wordvec in the single doc.\n",
        "def doc_vec(doc):  \n",
        "    doc = [word for word in doc] # target input list of words\n",
        "    return np.mean(wv[doc], axis = 0)\n",
        "\n",
        "vec = np.vstack(data.WordToken.apply(doc_vec))  #not sure abou whether a more efficient way"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcTmZTBc5b6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "le = LabelEncoder()\n",
        "#x_ = countvec\n",
        "#x_ = tfidfvec\n",
        "#x_ = w2vdvec\n",
        "x_ = vec\n",
        "lbl_ = le.fit_transform(data['TripStyle'])   # 1 for Leisure trip, 0 for Business\n",
        "#lbl_ = to_categorical(lbl_)\n",
        "#Split them into train/test set, randomly with the test size 0.33 \n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, lbl_train, lbl_test = train_test_split(x_, lbl_ , test_size = 0.33, shuffle = True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2a5aUXVOCMj",
        "colab_type": "code",
        "outputId": "921d8116-378f-46a5-8f6f-2c9691cc2116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#demision reduce:pca\n",
        "from sklearn.decomposition import PCA\n",
        "print(x_train.shape)\n",
        "pca = PCA(n_components=0.9)  \n",
        "x_train = pca.fit_transform(x_train)  \n",
        "x_test = pca.transform(x_test) \n",
        "print(x_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(134174, 161)\n",
            "(134174, 119)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPo2Vr4aZXGM",
        "colab_type": "text"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFXQuExAJRZj",
        "colab_type": "text"
      },
      "source": [
        "naive_bayes and cnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUr8EEUogrJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import sklearn.metrics as sklm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkE-idTHZXGN",
        "colab_type": "code",
        "outputId": "7c01ebdd-b8f4-4f7f-e4b8-cace97c1f780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf_gnb = GaussianNB()\n",
        "clf_gnb.fit(x_train, lbl_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J7n-6npOVnS",
        "colab_type": "text"
      },
      "source": [
        "CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLd7yiJHOWnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D,Conv1D,MaxPooling1D\n",
        "from keras import backend as K\n",
        "from keras.layers import Input,  Embedding\n",
        "from keras.layers import Reshape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcgP6D7t4IYG",
        "colab_type": "code",
        "outputId": "094c6147-f9eb-4182-dcd7-144ba1c8c2da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "#維度調整\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1],1,1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_train.shape[1] ,1, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print('x_train shape :',x_train.shape)\n",
        "print(x_train.shape[0] ,'train_samples')\n",
        "print(x_test.shape[0] ,'train_samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape : (134174, 249, 1, 1)\n",
            "134174 train_samples\n",
            "66087 train_samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjr__yRGOndJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 2\n",
        "epochs = 6\n",
        "inputshape=(x_train.shape[1],1,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6xb3O2ZOnnA",
        "colab_type": "code",
        "outputId": "afefe59e-735b-4e54-cdd6-57223f76dcce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(1, kernel_size=(1,1),\n",
        "                activation ='relu',\n",
        "                input_shape=inputshape))\n",
        "model.add(Conv2D(1,(1,1),\n",
        "                 activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1,1)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0614 02:57:37.511336 140623698663296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0614 02:57:37.566190 140623698663296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0614 02:57:37.577272 140623698663296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0614 02:57:37.619737 140623698663296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0614 02:57:37.623108 140623698663296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0614 02:57:37.630856 140623698663296 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQivXYfRsDiP",
        "colab_type": "code",
        "outputId": "dae2713a-e72c-454b-bdcc-939bf12775cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 249, 1, 1)         2         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 249, 1, 1)         2         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 249, 1, 1)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 249, 1, 1)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 249)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32000     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 32,262\n",
            "Trainable params: 32,262\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qZNgWh4PemV",
        "colab_type": "code",
        "outputId": "fc878626-049a-4960-b007-f76c936714d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "print(\"Traning Model...\")\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "             optimizer=keras.optimizers.Adadelta(),\n",
        "             metrics=['acc'])\n",
        "model.fit(x_train,lbl_train,\n",
        "         batch_size=batch_size,\n",
        "         epochs=epochs,\n",
        "         verbose =1,\n",
        "         # callbacks=[],\n",
        "         validation_data=(x_test,lbl_test))\n",
        "score= model.evaluate(x_test,lbl_test,verbose=0)\n",
        "print('Test loss:',score[0])\n",
        "print('Test accuracy:',score[1])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0614 02:57:47.716515 140623698663296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0614 02:57:47.732040 140623698663296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0614 02:57:47.876490 140623698663296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traning Model...\n",
            "Train on 134174 samples, validate on 66087 samples\n",
            "Epoch 1/6\n",
            "134174/134174 [==============================] - 13s 101us/step - loss: 0.4967 - acc: 0.8329 - val_loss: 0.4514 - val_acc: 0.8329\n",
            "Epoch 2/6\n",
            "134174/134174 [==============================] - 6s 41us/step - loss: 0.4502 - acc: 0.8336 - val_loss: 0.4513 - val_acc: 0.8329\n",
            "Epoch 3/6\n",
            "134174/134174 [==============================] - 6s 42us/step - loss: 0.4502 - acc: 0.8336 - val_loss: 0.4513 - val_acc: 0.8329\n",
            "Epoch 4/6\n",
            "134174/134174 [==============================] - 6s 42us/step - loss: 0.4502 - acc: 0.8336 - val_loss: 0.4513 - val_acc: 0.8329\n",
            "Epoch 5/6\n",
            "134174/134174 [==============================] - 6s 41us/step - loss: 0.4502 - acc: 0.8336 - val_loss: 0.4513 - val_acc: 0.8329\n",
            "Epoch 6/6\n",
            "134174/134174 [==============================] - 6s 42us/step - loss: 0.4502 - acc: 0.8336 - val_loss: 0.4513 - val_acc: 0.8329\n",
            "Test loss: 0.4512647079181229\n",
            "Test accuracy: 0.8329020836170502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU6h_tajmh_O",
        "colab_type": "code",
        "outputId": "954e4a64-7fc0-44e4-c5ca-91ff507a57e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(x_test, lbl_test, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66087/66087 [==============================] - 3s 51us/step\n",
            "Accuracy: 83.290208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBTIxDGGmpya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lbl_pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRkMWyf6muut",
        "colab_type": "code",
        "outputId": "627525be-dd50-4c75-9d90-f96e10298b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "lbl_pred1 = np.where(lbl_pred > 0.8 , 1, 0)\n",
        "print(classification_report(lbl_test, lbl_pred1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00     11043\n",
            "           1       0.83      1.00      0.91     55044\n",
            "\n",
            "   micro avg       0.83      0.83      0.83     66087\n",
            "   macro avg       0.42      0.50      0.45     66087\n",
            "weighted avg       0.69      0.83      0.76     66087\n",
            " samples avg       0.83      0.83      0.83     66087\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I70GzH6KTcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYBVV8mrZXGY",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18PN2_-cJajT",
        "colab_type": "text"
      },
      "source": [
        "for naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuAgVWoVZXGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluation function:\n",
        "from sklearn.metrics import precision_recall_curve, auc, confusion_matrix, accuracy_score, classification_report\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#ref: https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used\n",
        "def evaluating(truth, pred, ax=object):\n",
        "  \n",
        "    print(accuracy_score(truth, pred))\n",
        "    print(classification_report_imbalanced(truth, pred))    \n",
        "    print(confusion_matrix(truth, pred))\n",
        "    precision, recall, threshold = precision_recall_curve(truth, pred)\n",
        "\n",
        "    ax.step(recall, precision, color='b', alpha=1, where='post')\n",
        "    ax.fill_between(recall, precision, step='post', alpha=0.5, color='b')\n",
        "    ax.set_xlabel('Recall')\n",
        "    ax.set_ylabel('Precision')\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_title('Precision-Recall curve')\n",
        "    return ax\n",
        "#ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvJsX-J1ZXGb",
        "colab_type": "code",
        "outputId": "a712b575-d0fe-42d7-ed01-16352741eea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
        "\n",
        "pred_gnb = clf_gnb.predict(x_test)\n",
        "evaluating(lbl_test, pred_gnb, ax1)\n",
        "\n",
        "#pred_rf = clf_rf.predict(x_test)\n",
        "#evaluating(lbl_test, pred_rf, ax2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7830738269251138\n",
            "                   pre       rec       spe        f1       geo       iba       sup\n",
            "\n",
            "          0       0.26      0.17      0.90      0.21      0.39      0.14     10860\n",
            "          1       0.85      0.90      0.17      0.87      0.39      0.17     55227\n",
            "\n",
            "avg / total       0.75      0.78      0.29      0.76      0.39      0.16     66087\n",
            "\n",
            "[[ 1862  8998]\n",
            " [ 5338 49889]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe440add278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtgAAAGDCAYAAAARcmesAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wdG1lAKiOMMGgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXxG0NEZryLH",
        "colab_type": "text"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-8mjrjWr_Yd",
        "colab_type": "text"
      },
      "source": [
        "本次實驗嘗試進行了CNN的model和使用POStag，在naive bayes的模型下\n",
        "\n",
        "\n",
        "TFIDF+POStag得到的結果:0.61\n",
        "TFIDF+PCA:0.76\n",
        "\n",
        "CNN的模型成果如下\n",
        "\n",
        "TFIDF:0.834\n",
        "\n",
        "w2vec:0.835\n",
        "\n",
        "而NamedEntity的進行較不順利，使用nltk在轉換的過程中會遇到tee too deeply to convert的狀況因此無法轉換，而spacy的方法也會用完記憶體導致無法後續的進行"
      ]
    }
  ]
}